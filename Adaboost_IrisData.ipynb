{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Adaboost_IrisData.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOHuurw+nGZF8YCf7CQPbLx"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FAAtQuvMsDn5"},"source":["# **AdaBoost - Boosting Technique Machine Learning**"]},{"cell_type":"markdown","metadata":{"id":"V9Qb-57DsXfZ"},"source":["Adaboost which is otherwise called as Adaptive Boosting and this is supervised ML technique. This helps us to combine all weak learners into a single strong learner. The base learner/classifier could be of any of our basic classifiers, from decision tree (often default) to logistic regression, etc. If the classifier used in this technique is decision tree, then it will create different decision trees with one depth or single split, called ‘Decision Stump’. The number of decision stumps will depend on the number of features in the dataset. When there are M number of features in the dataset, this algorithm will create ‘M’ number of decision stumps. \n"," \n","1.\tThe first weak learner/classifier from for example, three weak learners sequentially, which we need to create from the dataset with the help of decision tree. All the base learners are decision trees. When we think about decision tree, it is not like the decision tree in the random forest. Instead, we need to create decision tree with only one depth / single split. These decision trees are basically called ‘Decision Stumps’\n","\n","2.\tInitially we assign weight that is  w = 1/n (n – total number of samples in the dataset) in all the samples. For example, above in the picture, total number of samples = 10 and the initial weight is 1 / 10\n","3.\tWe will create this stump for each and every feature in the dataset\n","4.\tWe will select the first decision tree base model. How to select the first decision tree base model? You know that we have two properties of decision tree, entropy and  Gini index / coefficient. After comparing all the features, whichever entropy has less value, we will select that decision tree as the first base learner model in the sequence base models\n","5.\tAfter the first base learner model built, we need to check the observations of misclassified, ie, how many records have been misclassified. For example, for the first feature, there are 10 (N) observations, and out of N observations, how many are misclassified (T), So, the total error (TE) is T / N  ( N – total number of observations, T – total number of misclassified observations. For example, the total number of records is 10 and the total number of misclassified record is 3, then Total Error is 3/10 = 0.3\n","6.\tWe need to find out the ‘Performance of Stump’ – that means how the stump has basically classified. Performance of Stump = ½ loge ((1-TE)/TE). For example, the initial weight is 3/10\n","Performance of Stump = ½ loge ((1-3/10)/(3/10)) = ½ loge (2.3333) = ½ * 0.8472 = 0.4236\n","\n","7.\tAfter calculating the Total Error and Performance of Stump, we need to update the weight for each and every record. So, first update the weight for the incorrectly classified samples and correctly classified samples in the dataset. In order to update the weight for the incorrectly classifier record, we will use the simple formula. \n","\n","The formal is New sample weight = old weight * ePerformance of Stump\n","New sample weight = 3/10 * e0.4236              \n","\t\t        = 0.4583\n","Update the new weight 0.4583 to the incorrectly classified samples which basically have increased the weight. \n","8.\tCalculate the new weight for the correctly classified samples. \n","The formula is New sample weight = old weight * e-Performance of Stump \n","                           New sample weight = 3/10 * e-0.4236             \n","\t\t\t\t         = 0.1964\n","9.\tUpdate the new weight 0.1286 for the correctly classified samples. \n","10.\tSummation of all these updated weight is not 1, whereas the summation of initial weight of all records is 1.            \n","So, we need to normalize the updated weights for all the samples.  \n","Normalized weight \t= \tUpdated weight / Total updated weight \n","\t\t\t= \t0.4583 / 2.7497 = 0.17   ----- for the incorrectly classified record\n","\t\t\t=\t0.1964 / 2.7497 = 0.07 -----------for the correctly classified record\n","So, the summation of all normalized weights is 1.\n","\n","11.\tAfter this, we have to make our second decision stump. For this, we will make a class intervals for the normalized weights. \n","\n","12.\tAfter that, we want to make a second weak model. But to do that, we need a sample dataset on which the second weak model can be run. For making it, we will run N number of iterations. On each iteration, it will calculate a random number ranging between 0-1 and this random will be compared with class intervals we created and on which class interval it lies, that row will be selected for sample data set. So new sample data set would also be of N observation. \n","\n","13.\tThis whole process will continue for M decision stumps. The final sequential tree would be considered as the final tree.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"WG8jFRuJr68J","executionInfo":{"status":"ok","timestamp":1603536129524,"user_tz":-330,"elapsed":1097,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Importing libraries\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.datasets import load_iris"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"bv2hAe8HtH3w","executionInfo":{"status":"ok","timestamp":1603536131002,"user_tz":-330,"elapsed":780,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}},"outputId":"3463bb77-dabe-421e-f564-4dba19eee81f","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["### Getting the data from IRIS dataset from sklearn\n","\n","iris = load_iris()\n","x = iris.data\n","y = iris.target\n","x"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]])"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"Pv-Etu4NtgQ5","executionInfo":{"status":"ok","timestamp":1603536132663,"user_tz":-330,"elapsed":703,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}},"outputId":"7cded8b9-e132-49f4-f4c3-b2ed37a804c7","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x.shape"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150, 4)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"oa3Z35P89H_M","executionInfo":{"status":"ok","timestamp":1603536134604,"user_tz":-330,"elapsed":1226,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}},"outputId":"76ca24ce-c2a1-4904-f4db-748408ef5bf1","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y.shape"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150,)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"kOhIRr1vvW2R","executionInfo":{"status":"ok","timestamp":1603536136056,"user_tz":-330,"elapsed":787,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}},"outputId":"48c6d1b5-ab21-4a5e-85a5-1519506a0039","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["y"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"RAoKtVB6tkQ5","executionInfo":{"status":"ok","timestamp":1603536137668,"user_tz":-330,"elapsed":818,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Splitting the data\n","\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrqsKlJ3KnII","executionInfo":{"status":"ok","timestamp":1603536139248,"user_tz":-330,"elapsed":717,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":[""],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"swcd5yEAuDvz","executionInfo":{"status":"ok","timestamp":1603536140767,"user_tz":-330,"elapsed":691,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### AdaBoost Classifier\n","\n","abc = AdaBoostClassifier(n_estimators=50, \n","                         learning_rate=1)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHecu3Y2ugDA","executionInfo":{"status":"ok","timestamp":1603536142634,"user_tz":-330,"elapsed":1088,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Fittng the model - Training the Adaboost classifier\n","\n","model = abc.fit(x_train, y_train)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjK4fi9JulxQ","executionInfo":{"status":"ok","timestamp":1603536144177,"user_tz":-330,"elapsed":848,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Predict the reponse for the test dataset\n","\n","y_pred = model.predict(x_test)"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8s82Rur9n--","executionInfo":{"status":"ok","timestamp":1603536145680,"user_tz":-330,"elapsed":694,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Evaluating the model\n","\n","from sklearn import metrics\n","accuracy = metrics.accuracy_score(y_test, y_pred)"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORwsTnc099eS","executionInfo":{"status":"ok","timestamp":1603536147101,"user_tz":-330,"elapsed":718,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}},"outputId":"1c3eef07-3455-4715-b0ae-df1ac25a0fbe","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["accuracy"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9333333333333333"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"89wZkkT2LCvV"},"source":["### **Using Logistic Regression as weak learner**"]},{"cell_type":"code","metadata":{"id":"4UoPSupt9_aK","executionInfo":{"status":"ok","timestamp":1603536156527,"user_tz":-330,"elapsed":1062,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### importing Logistic regression \n","\n","from sklearn.linear_model import LogisticRegression\n","\n","logisticRegressionModel = LogisticRegression()"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"wj6ITchWLbPz","executionInfo":{"status":"ok","timestamp":1603536158216,"user_tz":-330,"elapsed":1035,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Creating adaboost classifier object\n","\n","ada_Logistic = AdaBoostClassifier(n_estimators=50, learning_rate=1, base_estimator=logisticRegressionModel)"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9pxLXvKL4Zs","executionInfo":{"status":"ok","timestamp":1603536202826,"user_tz":-330,"elapsed":1142,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Train adaboost classifier\n","\n","model = ada_Logistic.fit(x_train, y_train)"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRPXanFYMf_T","executionInfo":{"status":"ok","timestamp":1603536240873,"user_tz":-330,"elapsed":1063,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}}},"source":["### Predict test data\n","\n","y_pred = model.predict(x_test)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZ7yyD3ANTyF","executionInfo":{"status":"ok","timestamp":1603536297914,"user_tz":-330,"elapsed":1031,"user":{"displayName":"BaluGDB","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjN4w1CaSwKdklrvs6k0Gp9BUWHd5y83Lrte9xP=s64","userId":"12073958349179601206"}},"outputId":"eaa0554d-4a87-4109-86ca-d13a0e33c359","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["### Evaluating the model\n","\n","metrics.accuracy_score(y_test, y_pred)\n","accuracy\n"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9333333333333333"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"FSX3ykfVNzf-"},"source":["**Advantages:**\n","\n","1.\tIt is fast, simple and easy to program\n","2.\tIt can be used to improve the accuracy of weak classifiers\n","3.  It is not prone to overfitting\n","**Disadvantages:**\n","1.\tIt is extremely sensitive to Noisy data and outliers. If you want to use AdaBoost, then it is recommended to remove noisy data and outliers. \n","2.\tComparatively, AdaBoost is slower than XgBoost"]}]}